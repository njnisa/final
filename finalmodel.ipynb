{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import joblib\n",
    "import warnings\n",
    "import pickle\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "data = pd.read_csv(\"ha.csv\")\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "\n",
    "\n",
    "\n",
    "#dropping the columns with missing value\n",
    "df.dropna()\n",
    "\n",
    "#Renaming the columns names\n",
    "df.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n",
    "       'exercise_induced_angina', 'st_depression', 'st_slope','target']\n",
    "df\n",
    "\n",
    "#description of each column\n",
    "data.describe()\n",
    "\n",
    "#removal of outliers\n",
    "'''q1 = data.resting_blood_pressure.quantile(0.25)\n",
    "q2 = data.resting_blood_pressure.quantile(0.75)\n",
    "print(q1,q2)\n",
    "IQR1 = q2-q1\n",
    "print(IQR1)\n",
    "lower_limit = q1-1.5*IQR1\n",
    "upper_limit = q2+1.5*IQR1\n",
    "print(lower_limit,upper_limit)\n",
    "\n",
    "q3 = data.cholesterol.quantile(0.25)\n",
    "q4 = data.cholesterol.quantile(0.75)\n",
    "print(q3,q4)\n",
    "IQR2 = q4-q3\n",
    "print(IQR2)\n",
    "lower_limit1 = q3-1.5*IQR2\n",
    "upper_limit1= q4+1.5*IQR2\n",
    "print(lower_limit1,upper_limit1)\n",
    "\n",
    "q5 = data.max_heart_rate_achieved.quantile(0.25)\n",
    "q6 = data.max_heart_rate_achieved.quantile(0.75)\n",
    "print(q5,q6)\n",
    "IQR3 = q6-q5\n",
    "print(IQR3)\n",
    "lower_limit2 = q5-1.5*IQR3\n",
    "upper_limit2 = q6+1.5*IQR3\n",
    "print(lower_limit2,upper_limit2)\n",
    "\n",
    "df_no_outlier =data[(data.resting_blood_pressure>lower_limit)&(data.resting_blood_pressure<upper_limit)&(data.cholesterol>lower_limit1)&(data.cholesterol<upper_limit1)&(data.max_heart_rate_achieved>lower_limit2)&(data.max_heart_rate_achieved<upper_limit2)]\n",
    "df_no_outlier'''\n",
    "\n",
    "numeric_features = ['age', 'resting_blood_pressure', 'cholesterol', 'max_heart_rate_achieved', 'st_depression']\n",
    "\n",
    "eda_df = data.loc[:, numeric_features].copy()\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "for i in range(len(eda_df.columns)):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    sns.boxplot(eda_df[eda_df.columns[i]])\n",
    "\n",
    "plt.show()\n",
    "corr = eda_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, annot=True, vmin=-1.0, cmap='mako')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(data['target'].value_counts(), labels=[\"Heart Disease\", \"No Heart Disease\"], autopct='%.1f%%', colors=['#36a2ac', '#413f80'])\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.show()\n",
    "def onehot_encode(df, column_dict):\n",
    "    df = df.copy()\n",
    "    for column, prefix in column_dict.items():\n",
    "        dummies = pd.get_dummies(df[column], prefix=prefix)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        df = df.drop(column, axis=1)\n",
    "    return df\n",
    "def preprocess_inputs(df, scaler):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # One-hot encode the nominal features\n",
    "    nominal_features = ['chest_pain_type', 'st_slope']\n",
    "    df = onehot_encode(df, dict(zip(nominal_features, ['CP', 'SL'])))\n",
    "    \n",
    "    # Split df into X and y\n",
    "    y = df['target'].copy()\n",
    "    X = df.drop('target', axis=1).copy()\n",
    "    \n",
    "    # Scale X\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    return X, y\n",
    "X, y = preprocess_inputs(data, RobustScaler())\n",
    "#X = df.drop('target',axis=1)\n",
    "#y = df['target']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n",
    "\n",
    "def get_stacking():\n",
    "    # define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('lr', LogisticRegression(C= 1.0, penalty= 'l2', solver= 'newton-cg')))\n",
    "    level0.append(('knn', KNeighborsClassifier(metric= 'manhattan', n_neighbors= 17, weights= 'distance')))\n",
    "    level0.append(('cart', DecisionTreeClassifier(criterion= 'entropy', max_depth= 12, max_features= 'auto', min_samples_leaf= 1, min_samples_split= 2)))\n",
    "    level0.append(('svm',SVC(C= 1000, gamma= 0.0001, kernel= 'rbf')))\n",
    "    level0.append(('bayes', GaussianNB()))\n",
    "    level0.append(('rfa',RandomForestClassifier(max_features='sqrt', n_estimators= 100)))\n",
    "    level0.append(('neural',MLPClassifier(max_iter=1500)))    \n",
    "        # define meta learner model\n",
    "    level1 = LogisticRegression(C= 1.0, penalty= 'l2', solver= 'newton-cg')\n",
    "        # define the stacking ensemble\n",
    "    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model\n",
    "\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['lr'] = LogisticRegression()\n",
    "    models['knn'] = KNeighborsClassifier()\n",
    "    models['cart'] = DecisionTreeClassifier()\n",
    "    models['svm'] = SVC()\n",
    "    models['bayes'] = GaussianNB()\n",
    "    models['neural'] = MLPClassifier()  \n",
    "    models['stacking'] = get_stacking()\n",
    "    return models\n",
    "model=get_stacking()\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores\n",
    "\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "\n",
    "#Bagging \n",
    "#level1 = LogisticRegression(C= 1.0, penalty= 'l2', solver= 'newton-cg')\n",
    "model1= BaggingClassifier(n_estimators=500,max_samples=1.0)\n",
    "#model1 = StackingClassifier(estimators=level2, final_estimator=level1, cv=10)\n",
    "cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "n_scores = cross_val_score(model1, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "model2 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10),n_estimators=5000)\n",
    "# fit the model on the whole dataset\n",
    "cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "n_scores = cross_val_score(model2, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model3 = RandomForestClassifier(max_features='sqrt', n_estimators= 100)\n",
    "cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "n_scores = cross_val_score(model2, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def get_voting():\n",
    "    # define the base models\n",
    "    models = list()\n",
    "    models.append(('stacking', model))\n",
    "    models.append(('bagging',  model1))\n",
    "    models.append(('boosting',  model2))\n",
    "    models.append(('rf',model3))\n",
    "    # define the voting ensemble\n",
    "    ensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "    return ensemble\n",
    " \n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['stacking'] = model\n",
    "    models['bagging'] = model1\n",
    "    models['boosting'] = model2\n",
    "    models['rf'] = model3\n",
    "    models['hard_voting'] = get_voting()\n",
    "    return models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores\n",
    " \n",
    "# define dataset\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "final = models['hard_voting']\n",
    "final.fit(X_train, y_train)\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get a list of base models\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=1)\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(('stacking',model))\n",
    "\tmodels.append(('bagging',model1))\n",
    "\tmodels.append(('boosting',model2))\n",
    "\treturn models\n",
    "\n",
    "# evaluate each base model\n",
    "def evaluate_models(models, X_train1, X_val, y_train1, y_val):\n",
    "\t# fit and evaluate the models\n",
    "\tscores = list()\n",
    "\tfor name, model in models:\n",
    "\t\t# fit the model\n",
    "\t\tmodel.fit(X_train1, y_train1)\n",
    "\t\t# evaluate the model\n",
    "\t\tyhat = model.predict(X_val)\n",
    "\t\tacc = accuracy_score(y_val, yhat)\n",
    "\t\t# store the performance\n",
    "\t\tscores.append(acc)\n",
    "\t\t# report model performance\n",
    "\treturn scores\n",
    " \n",
    "# define dataset\n",
    "#X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# split dataset into train and test sets\n",
    "#X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.50, random_state=1)\n",
    "# split the full train set into train and validation sets\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.33, random_state=1)\n",
    "# create the base models\n",
    "models = get_models()\n",
    "# fit and evaluate each model\n",
    "scores = evaluate_models(models, X_train1, X_val, y_train1, y_val)\n",
    "print(scores)\n",
    "# create the ensemble\n",
    "ensemble = VotingClassifier(estimators=models, voting='hard', weights=scores)\n",
    "# fit the ensemble on the training dataset\n",
    "ensemble.fit(X_train, y_train)\n",
    "# make predictions on test set\n",
    "yhat = ensemble.predict(X_test)\n",
    "# evaluate predictions\n",
    "score = accuracy_score(y_test, yhat)\n",
    "print('Weighted Avg Accuracy: %.3f' % (score*100))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
